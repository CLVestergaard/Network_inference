{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node sampling: induced subgraph sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def induced_subgraph_sampling(G, n):\n",
    "    \"\"\"\n",
    "    Samples an induced subgraph of `n` nodes from the input graph `G`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph\n",
    "        The input graph from which the subgraph is sampled.\n",
    "    n : int\n",
    "        The number of nodes to include in the sampled induced subgraph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    subgraph : networkx.Graph\n",
    "        The induced subgraph with `n` nodes and the edges among them.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `n` is larger than the number of nodes in the graph.\n",
    "    \"\"\"\n",
    "    # Ensure `n` is not larger than the number of nodes in G\n",
    "    if n > G.number_of_nodes():\n",
    "        raise ValueError(\"`n` must be less than or equal to the number of nodes in the graph.\")\n",
    "    \n",
    "    # Get the list of nodes in the graph\n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    # Randomly sample `n` nodes without replacement\n",
    "    sampled_nodes = np.random.choice(nodes, size=n, replace=False)\n",
    "    \n",
    "    # Create an induced subgraph of G using the sampled nodes\n",
    "    subgraph = G.subgraph(sampled_nodes).copy()\n",
    "    \n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate an Erdos-Renyi (ER) and Watts-Strogatz (WS) network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "M = 50000\n",
    "average_degree = 2*M/N\n",
    "print(average_degree)\n",
    "\n",
    "p = average_degree/(N-1)\n",
    "G_er = nx.erdos_renyi_graph(N,p)\n",
    "G_ws = nx.watts_strogatz_graph(N, int(average_degree), p=0.01)\n",
    "\n",
    "print(\"Average degree ER:\", np.mean(list(dict(G_er.degree()).values())))\n",
    "print(\"Average degree WS:\",  np.mean(list(dict(G_ws.degree()).values())))\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(8,4),dpi=200)\n",
    "cols = ['steelblue','mediumseagreen']\n",
    "\n",
    "ax[0].hist(dict(G_er.degree()).values(),color=cols[0])\n",
    "ax[0].set_xlabel(r'$k_i$')\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "ax[1].hist(dict(G_ws.degree()).values(),color=cols[1])\n",
    "ax[1].set_xlabel(r'$k_i$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample different fractions of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.circular_layout(G_er)\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(18,4),dpi=200)\n",
    "\n",
    "for ni,n in enumerate([50,100,200,500]):\n",
    "    subg = induced_subgraph_sampling(G_er, n)\n",
    "    nx.draw(G_er, pos, node_color='.4', node_size=50, alpha=0.5, edge_color='.6', width=0.6, ax=ax[ni])\n",
    "    nx.draw(subg, pos, node_color=cols[0],node_size=20, edge_color=cols[0], width=0.6, ax=ax[ni])\n",
    "    ax[ni].set_title(r'$n = %i$, $\\langle k \\rangle = %.1f$'%(n, np.mean(list(dict(subg.degree).values()))))\n",
    "\n",
    "plt.suptitle(r'Induced Subgraph Sampling (ER network, N=%i, $\\langle k \\rangle = %.1f$)'%(\n",
    "    G_ws.number_of_nodes(),average_degree),\n",
    "             y=1.05, fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4,figsize=(18,4),dpi=200)\n",
    "\n",
    "for ni,n in enumerate([50,100,200,500]):\n",
    "    subg = induced_subgraph_sampling(G_er, n)    \n",
    "    ax[ni].hist(dict(subg.degree()).values(),color=cols[0])\n",
    "    ax[ni].set_xlabel(r'$k$')\n",
    "\n",
    "ax[0].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nx.circular_layout(G_ws)\n",
    "\n",
    "fig, ax = plt.subplots(1,4,figsize=(18,4),dpi=200)\n",
    "\n",
    "for ni,n in enumerate([50,100,200,500]):\n",
    "    subg = induced_subgraph_sampling(G_ws, n)\n",
    "    nx.draw(G_ws, pos, node_color='.4', node_size=50, alpha=0.5, edge_color='.6', width=0.6, ax=ax[ni])\n",
    "    nx.draw(subg, pos, node_color=cols[1],node_size=20, edge_color=cols[1], width=0.6, ax=ax[ni])\n",
    "    ax[ni].set_title(r'$n = %i$, $\\langle k \\rangle = %.1f$'%(n, np.mean(list(dict(subg.degree).values()))))\n",
    "\n",
    "plt.suptitle(r'Induced Subgraph Sampling (Watts-Strogatz network, N=%i, $\\langle k \\rangle = %.1f$)'%(\n",
    "    G_ws.number_of_nodes(),average_degree),\n",
    "             y=1.05, fontsize='xx-large')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,4,figsize=(18,4),dpi=200)\n",
    "\n",
    "for ni,n in enumerate([50,100,200,500]):\n",
    "    subg = induced_subgraph_sampling(G_ws, n)    \n",
    "    ax[ni].hist(dict(subg.degree()).values(),color=cols[1])\n",
    "    ax[ni].set_xlabel(r'$k$')\n",
    "\n",
    "ax[0].set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate average degree using the Horvitz-Thompson estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(11,4),dpi=200,sharey=True,sharex=True)\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "    \n",
    "B = 200 # number of simulations (independent node sampling rounds)\n",
    "labels = ['ER','WS']\n",
    "for a, G in enumerate([G_er, G_ws]):\n",
    "    av_degs = np.zeros(B)\n",
    "    for b in range(B):\n",
    "        n = 50\n",
    "        subg_G = induced_subgraph_sampling(G, n)\n",
    "        m = subg_G.number_of_edges()\n",
    "        prob_e = n*(n-1)/N/(N-1)\n",
    "        M_est = m/prob_e\n",
    "        av_degs[b] = 2*M_est/N\n",
    "\n",
    "\n",
    "    ax[a].hist(av_degs, 20, label=labels[a], color=cols[a], ec='.2', alpha=0.7)\n",
    "    degs_G = np.mean(list(dict(G.degree()).values()))\n",
    "    ax[a].vlines(degs_G, 0, 35, lw=3, label='True value', color='orange')\n",
    "\n",
    "    mu = np.mean(av_degs)\n",
    "    ax[a].vlines(mu, 0, 35, color='.1', lw=2, label='Estimated value', ls='--')\n",
    "    ax[a].legend()\n",
    "    ax[a].set_ylim(0,35)\n",
    "    ax[a].set_xlabel(r'$\\hat{\\overline{k}}$')\n",
    "\n",
    "ax[0].set_ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the number of nodes in a graph\n",
    "The _capture-recapture_ estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 100  # Number of iterations\n",
    "\n",
    "nodes = list(range(N))  \n",
    "\n",
    "# Array to store network size estimates\n",
    "N_est = np.zeros(B)\n",
    "\n",
    "### Perform capture-recapture simulation\n",
    "for b in range(B):\n",
    "    # Randomly sample the size of the first and second sample sets\n",
    "    n1 = np.random.randint(150, 200)\n",
    "    n2 = np.random.randint(150, 200)\n",
    "    \n",
    "    # Randomly select nodes for the first and second samples\n",
    "    V_s1 = np.random.choice(nodes, size=n1, replace=False)\n",
    "    V_s2 = np.random.choice(nodes, size=n2, replace=False)\n",
    "    \n",
    "    # Count the number of nodes in the intersection (recaptured nodes)\n",
    "    n_c = len(set(V_s1).intersection(V_s2))\n",
    "    \n",
    "    # Estimate the network size if recaptured nodes exist; otherwise, set to NaN\n",
    "    N_est[b] = n1 * n2 / n_c if n_c > 0 else np.nan\n",
    "\n",
    "\n",
    "# Filter out invalid estimates (NaN values)\n",
    "N_est = N_est[np.isfinite(N_est)]\n",
    "\n",
    "#################\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=150)\n",
    "ax.hist(N_est, bins=25, color='.8', edgecolor='.2', alpha=0.7)\n",
    "ax.vlines(N, 0, 15, linewidth=3, label='True value', color='orange')\n",
    "\n",
    "mu = np.mean(N_est)\n",
    "ax.vlines(mu, 0, 15, color='.1', linewidth=2, label='Estimated value', linestyle='--')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_yticks(range(0,25,5))\n",
    "ax.set_ylim(0, 15)\n",
    "ax.set_xlabel(r'$\\hat{N}$')\n",
    "ax.set_ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge sampling: Incedent subgraph sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def incident_subgraph_sampling(G, n, is_directed=False):\n",
    "    \"\"\"\n",
    "    Samples an incident subgraph containing `n` edges from the input graph `G`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    G : networkx.Graph or networkx.DiGraph\n",
    "        The input graph from which the subgraph is sampled.\n",
    "    n : int\n",
    "        The number of edges to include in the sampled incident subgraph.\n",
    "    is_directed : bool, optional (default=False)\n",
    "        Specifies if the graph is directed. If True, edges are treated as directed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    subgraph : networkx.Graph or networkx.DiGraph\n",
    "        The incident subgraph with `n` edges and the nodes connected by those edges.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `n` is larger than the number of edges in the graph.\n",
    "    \"\"\"\n",
    "    # Ensure `n` is not larger than the number of edges in G\n",
    "    if n > G.number_of_edges():\n",
    "        raise ValueError(\"`n` must be less than or equal to the number of edges in the graph.\")\n",
    "    \n",
    "    # Get the list of edges in the graph\n",
    "    edges = list(G.edges())\n",
    "    \n",
    "    # Shuffle the edges randomly to sample from them\n",
    "    shuffle(edges)\n",
    "    \n",
    "    # Select the first `n` edges from the shuffled list\n",
    "    sampled_edges = edges[:n]\n",
    "    \n",
    "    # Find the unique nodes connected by the sampled edges\n",
    "    sampled_nodes = set(node for edge in sampled_edges for node in edge)\n",
    "    \n",
    "    # Create a new subgraph with the sampled nodes and edges\n",
    "    subgraph = G.edge_subgraph(sampled_edges).copy()\n",
    "    \n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating fractions of nodes belonging to two groups in Albert-Barabasi graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "G = nx.barabasi_albert_graph(N,10)\n",
    "pos = nx.spring_layout(G)\n",
    "print(\"Average degree: %1.1f\" % np.mean(list(dict(G.degree()).values())))\n",
    "M = G.number_of_edges()\n",
    "print(\"Number of edges:\", M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_red = 0.7\n",
    "reds = 0\n",
    "blues = 0\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['team'] = 'red' if np.random.rand() < prob_red else 'blue'\n",
    "    if G.nodes[node]['team'] == 'red':\n",
    "        reds += 1\n",
    "    else:\n",
    "        blues += 1\n",
    "        \n",
    "print(\"Number of red nodes:\",reds)\n",
    "print(\"Number of blue nodes:\",blues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(5,5),dpi=100)\n",
    "nx.draw(G, pos=pos, node_color=nx.get_node_attributes(G, 'team').values(),ax=ax,node_size=50, edge_color='.6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate fractions of node colors from sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import binom\n",
    "\n",
    "def prob_i(N_e, k_i, n):\n",
    "    \"\"\"\n",
    "    Calculate the probability of at least one sample overlapping with node i.\n",
    "\n",
    "    Parameters:\n",
    "    N_e : int\n",
    "        Effective size of the network.\n",
    "    k_i : int\n",
    "        Number of samples that do not include node i.\n",
    "    n : int\n",
    "        Number of nodes in the sample.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        Probability of at least one sample overlapping with node i.\n",
    "    \"\"\"\n",
    "    # If the sample size exceeds the available nodes excluding k_i, \n",
    "    # the probability is 1 (certain inclusion).\n",
    "    if n > N_e - k_i:\n",
    "        return 1.0\n",
    "    \n",
    "    # Otherwise, calculate the complement of the probability that node i is not sampled.\n",
    "    return 1.0 - binom(N_e - k_i, n) / binom(N_e, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Horovitz-Thompson estimator to get those numbers from sampled graphs\n",
    "B = 1000\n",
    "reds_est = [None]*B\n",
    "blues_est = [None]*B\n",
    "degrees = dict(G.degree())\n",
    "\n",
    "for b in range(B):\n",
    "    n = 100\n",
    "    G_s = incident_subgraph_sampling(G,n)\n",
    "    V_s = list(G_s.nodes())\n",
    "    E_s = list(G_s.edges())\n",
    "    pi = {node: prob_i(M,degrees[node],n)  for node in V_s}\n",
    "    reds_est[b] = sum([ 1.0/pi[node] for node in V_s if G.nodes[node]['team'] == 'red'])\n",
    "    blues_est[b] = sum([ 1.0/pi[node] for node in V_s if G.nodes[node]['team'] == 'blue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12,5),dpi=100)\n",
    "\n",
    "ax.hist(reds_est, bins=50, color='crimson', alpha=0.4, label='Estimated average reds: %.2f'%(np.mean(reds_est)))\n",
    "ax.hist(blues_est, bins=50, color='steelblue', alpha=0.4, label='Estimated average blues: %.2f'%(np.mean(blues_est)))\n",
    "\n",
    "ax.vlines(reds, 0, 100, color='crimson', ls='--', label='True reds: %i'%reds)\n",
    "ax.vlines(blues, 0, 100, color='steelblue', ls='--', label='True blues: %i'%blues)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "ax.set_ylim(0,100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate degree distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for calculating binned distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binning(data, num_bins=50, is_pmf=False, log_binning=False, threshold=0):\n",
    "    \"\"\"\n",
    "    Bins the input data and calculates the probability mass function (PMF) or \n",
    "    probability density function (PDF) over the bins. Supports both linear and \n",
    "    logarithmic binning.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array-like\n",
    "        The data to be binned, typically a list or numpy array of values.\n",
    "    num_bins : int, optional\n",
    "        The number of bins to use for binning the data (default is 15).\n",
    "    is_pmf : bool, optional\n",
    "        If True, computes the probability mass function (PMF) by normalizing \n",
    "        histogram counts to sum to 1. If False, computes the probability density \n",
    "        function (PDF) by normalizing the density of the bins (default is True).\n",
    "    log_binning : bool, optional\n",
    "        If True, uses logarithmic binning with log-spaced bins. If False, uses \n",
    "        linear binning (default is False).\n",
    "    threshold : float, optional\n",
    "        Only values greater than `threshold` will be included in the binning, \n",
    "        allowing for the removal of isolated nodes or outliers (default is 0).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : numpy.ndarray\n",
    "        The bin centers, adjusted to be the midpoint of each bin.\n",
    "    p : numpy.ndarray\n",
    "        The computed PMF or PDF values for each bin.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    This function removes values below a specified threshold, then defines \n",
    "    bin edges based on the specified binning method (linear or logarithmic). \n",
    "    It calculates either the PMF or PDF based on `is_pmf`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter out isolated nodes or low values by removing data below threshold\n",
    "    values = list(filter(lambda x: x > threshold, data))\n",
    "    if len(values) != len(data):\n",
    "        print(\"%s isolated nodes have been removed\" % (len(data) - len(values)))\n",
    "\n",
    "    # Define the range for binning (support of the distribution)\n",
    "    lower_bound = min(values)\n",
    "    upper_bound = max(values)\n",
    "\n",
    "    # Define bin edges based on binning type (logarithmic or linear)\n",
    "    if log_binning:\n",
    "        # Use log-spaced bins by taking the log of the bounds\n",
    "        lower_bound = np.log10(lower_bound)\n",
    "        upper_bound = np.log10(upper_bound)\n",
    "        bin_edges = np.logspace(lower_bound, upper_bound, num_bins + 1, base=10)\n",
    "    else:\n",
    "        # Use linearly spaced bins\n",
    "        bin_edges = np.linspace(lower_bound, upper_bound, num_bins + 1)\n",
    "\n",
    "    # Calculate histogram based on chosen binning method\n",
    "    if is_pmf:\n",
    "        # Calculate PMF: normalized counts of data in each bin\n",
    "        y, _ = np.histogram(values, bins=bin_edges, density=False)\n",
    "        p = y / y.sum()  # Normalize to get probabilities\n",
    "    else:\n",
    "        # Calculate PDF: normalized density of data in each bin\n",
    "        p, _ = np.histogram(values, bins=bin_edges, density=True)\n",
    "\n",
    "    # Compute bin centers (midpoints) to represent each bin\n",
    "    x = bin_edges[1:] - np.diff(bin_edges) / 2  # Bin centers for plotting\n",
    "\n",
    "    # Remove bins with zero probability to avoid plotting/display issues\n",
    "    x = x[p > 0]\n",
    "    p = p[p > 0]\n",
    "\n",
    "    return x, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulate microcanonical ER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 50000\n",
    "N = 10000\n",
    "G = nx.gnm_random_graph(N,M)\n",
    "degrees = list(dict(G.degree()).values())\n",
    "print(\"Average degree: %1.3f\" % np.mean(degrees))\n",
    "n = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Incident subgraph Sampling\n",
    "G_incid = incident_subgraph_sampling(G, n)\n",
    "degrees_incid = list(dict(G_incid.degree()).values())\n",
    "x_incid,y_incid = get_binning(degrees_incid, log_binning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Degree distribution\n",
    "x_true,y_true = get_binning(degrees, log_binning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(7,5),dpi=100)\n",
    "ax.loglog(x_true,y_true,'o', label='Original Graph')\n",
    "ax.loglog(x_incid, y_incid, 'o', label='Incident subgraph Sampling')\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$k$',fontsize='x-large')\n",
    "ax.set_ylabel(r'$P(k)$',fontsize='x-large')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Sampling with Graph Induction (ES-i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_sampling_with_induction(G, n):\n",
    "    \"\"\"\n",
    "    Perform edge sampling with induction and return a new graph.\n",
    "\n",
    "    Parameters:\n",
    "    G : networkx.Graph\n",
    "        Input graph.\n",
    "    n : int\n",
    "        Number of edges to sample initially.\n",
    "\n",
    "    Returns:\n",
    "    networkx.Graph\n",
    "        A new graph induced by the sampled nodes and edges.\n",
    "    \"\"\"\n",
    "    # Get all edges and total number of edges in the graph\n",
    "    edges = list(G.edges())\n",
    "    M = G.number_of_edges()\n",
    "    \n",
    "    # Sample n edges randomly without replacement\n",
    "    sampled_edges = set([edges[i] for i in np.random.permutation(M)[:n]])\n",
    "    \n",
    "    # Extract the set of nodes in the sampled edges\n",
    "    nodes_from_sampled_edges = set(node for edge in sampled_edges for node in edge)\n",
    "    \n",
    "    # Induce edges between the sampled nodes\n",
    "    induced_edges = set(sampled_edges)\n",
    "    for node_i, node_j in edges:\n",
    "        if node_i in nodes_from_sampled_edges and node_j in nodes_from_sampled_edges:\n",
    "            induced_edges.add((node_i, node_j))\n",
    "    \n",
    "    # Create a new graph with the sampled nodes and induced edges\n",
    "    new_graph = nx.Graph()\n",
    "    new_graph.add_nodes_from(nodes_from_sampled_edges)\n",
    "    new_graph.add_edges_from(induced_edges)\n",
    "    \n",
    "    return new_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_esi = edge_sampling_with_induction(G, n)\n",
    "degrees_esi = list(dict(G_esi.degree()).values())\n",
    "x_esi,y_esi = get_binning(degrees_esi, log_binning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(7,5),dpi=100)\n",
    "ax.loglog(x_true, y_true, 'o', label='Original Graph')\n",
    "ax.loglog(x_incid, y_incid, 'o', label='Incident Graph Sampling')\n",
    "ax.loglog(x_esi, y_esi, 'o', label='Edge-Sampling w/ Induction')\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$k$',fontsize='x-large')\n",
    "ax.set_ylabel(r'$P(k)$',fontsize='x-large')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative comparison of degree distributions using the Kolmogorov-Smirnov (KS) statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "def ks_d(values1, values2):\n",
    "    \"\"\"\n",
    "    Compute the Kolmogorov-Smirnov (KS) statistic to compare two distributions.\n",
    "\n",
    "    Parameters:\n",
    "    values1 : array-like\n",
    "        First sample of values.\n",
    "    values2 : array-like\n",
    "        Second sample of values.\n",
    "\n",
    "    Returns:\n",
    "    float\n",
    "        KS statistic (D), representing the maximum distance between the empirical \n",
    "        cumulative distribution functions (ECDFs) of the two samples.\n",
    "    \"\"\"\n",
    "    D, _ = ks_2samp(values1, values2)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original vs ES-i:', ks_d(degrees, degrees_esi))\n",
    "print('Original vs Incident Graph Sampling:', ks_d(degrees, degrees_incid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snowball_sampling(G, seed_nodes, n_waves=2):\n",
    "    \"\"\"\n",
    "    Perform snowball sampling on a graph.\n",
    "\n",
    "    Parameters:\n",
    "    G : networkx.Graph or networkx.DiGraph\n",
    "        Input graph.\n",
    "    seed_nodes : list or set\n",
    "        Initial set of nodes to start the snowball sampling.\n",
    "    n_waves : int, optional\n",
    "        Number of waves to expand the snowball sampling (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    networkx.Graph or networkx.DiGraph\n",
    "        A new graph G_s containing the sampled nodes and edges.\n",
    "    \"\"\"\n",
    "    # Initialize sets to keep track of sampled nodes and edges\n",
    "    V_s = set(seed_nodes)\n",
    "    E_s = set()\n",
    "    \n",
    "    # Initialize the wave structure\n",
    "    V = [set()] * (n_waves + 1)\n",
    "    V[0] = set(seed_nodes)\n",
    "    \n",
    "    # Perform snowball sampling over the specified number of waves\n",
    "    for k in range(n_waves):\n",
    "        V[k + 1] = set()  # Initialize the next wave\n",
    "        for node_i in V[k]:\n",
    "            for node_j in G.neighbors(node_i):\n",
    "                # Add the edge to the sampled edge set\n",
    "                edge = (node_i, node_j) if not G.is_directed() and node_i < node_j else (node_i, node_j)\n",
    "                E_s.add(edge)\n",
    "                \n",
    "                # Add the neighboring node to the next wave\n",
    "                V[k + 1].add(node_j)\n",
    "        \n",
    "        # Exclude nodes already in the sampled set\n",
    "        V[k + 1] -= V_s\n",
    "        \n",
    "        # Add new nodes to the sampled set\n",
    "        V_s.update(V[k + 1])\n",
    "    \n",
    "    # Create the new sampled graph\n",
    "    G_s = G.subgraph(V_s).copy()\n",
    "    return G_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Snowball sampling of BA graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15000\n",
    "m = 3\n",
    "G = nx.barabasi_albert_graph(N, m)\n",
    "M = G.number_of_nodes()\n",
    "degrees = list(dict(G.degree()).values())\n",
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10, 5), dpi=200)\n",
    "\n",
    "n = 50  # Number of seed nodes\n",
    "n_waves = 5  # Maximum number of waves\n",
    "degrees_sampled = [None] * (n_waves + 1)\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "\n",
    "# Perform snowball sampling for increasing numbers of waves\n",
    "for k in range(n_waves + 1):\n",
    "    # Select seed nodes randomly\n",
    "    seed_nodes = set(np.random.choice(nodes, size=n, replace=False))\n",
    "    \n",
    "    # Perform snowball sampling\n",
    "    G_s = snowball_sampling(G, seed_nodes, n_waves=k)\n",
    "    \n",
    "    # Print summary of the sampled graph\n",
    "    print(f\"Number of waves: {k} | Number of nodes discovered: {G_s.number_of_nodes()} | \"\n",
    "          f\"Number of edges discovered: {G_s.number_of_edges()}\")\n",
    "    \n",
    "    # Get degree distribution of the sampled graph\n",
    "    degrees_sampled[k] = [deg for _, deg in G_s.degree()]\n",
    "    \n",
    "    # Plot degree distribution for the current wave\n",
    "    if k > 0:\n",
    "        x1, y1 = get_binning(degrees_sampled[k], log_binning=True, num_bins=15, is_pmf=False)\n",
    "        ax.loglog(x1, y1, '--o', label=f'Wave: {k}')\n",
    "\n",
    "# Plot the degree distribution of the original graph\n",
    "x, y = get_binning(degrees, log_binning=True, num_bins=15, is_pmf=False)\n",
    "ax.scatter(x, y, s=100, facecolors='none', edgecolors='r', lw=3, label='Original Graph')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$k$',fontsize='x-large')\n",
    "ax.set_ylabel(r'$P(k)$',fontsize='x-large')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traceroute sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceroute_sampling(G, n_sources=5, n_targets=50):\n",
    "    \"\"\"\n",
    "    Perform traceroute sampling on a graph, selecting multiple sources and targets.\n",
    "\n",
    "    Parameters:\n",
    "    G : networkx.Graph\n",
    "        Input graph.\n",
    "    n_sources : int, optional\n",
    "        Number of source nodes (default is 5).\n",
    "    n_targets : int, optional\n",
    "        Number of target nodes (default is 50).\n",
    "\n",
    "    Returns:\n",
    "    networkx.Graph\n",
    "        A new graph containing the sampled nodes and edges.\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "    shuffle(nodes)\n",
    "\n",
    "    # Select sources and targets\n",
    "    sources = set(nodes[:n_sources])\n",
    "    targets = set(nodes[n_sources:n_sources + n_targets])\n",
    "\n",
    "    # Initialize the sampled graph\n",
    "    G_s = nx.Graph() if not G.is_directed() else nx.DiGraph()\n",
    "\n",
    "    # Compute shortest paths from each source to each target\n",
    "    for source_node in sources:\n",
    "        for target_node in targets:\n",
    "            try:\n",
    "                path = nx.shortest_path(G, source=source_node, target=target_node)\n",
    "                G_s.add_nodes_from(path)\n",
    "                G_s.add_edges_from((path[i], path[i + 1]) for i in range(len(path) - 1))\n",
    "            except nx.NetworkXNoPath:\n",
    "                pass  # Ignore if no path exists\n",
    "\n",
    "    return G_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Incident Graph Sampling\n",
    "G_tr = traceroute_sampling(G, n)\n",
    "degrees_tr = list(dict(G_tr.degree()).values())\n",
    "x_tr,y_tr = get_binning(degrees_tr, log_binning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(7,5),dpi=100)\n",
    "ax.loglog(x_true, y_true, 'o', label='Original Graph')\n",
    "# ax.loglog(x_incid, y_incid, 'o', label='Incident Graph Sampling')\n",
    "# ax.loglog(x_esi, y_esi, 'o', label='Edge-Sampling w/ Induction')\n",
    "ax.loglog(x_tr, y_tr, 'o', label='Traceroute-Sampling')\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$k$',fontsize='x-large')\n",
    "ax.set_ylabel(r'$P(k)$',fontsize='x-large')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph parameters\n",
    "N = 1000\n",
    "k = 15.0\n",
    "M = int(0.5 * N * k)\n",
    "G = nx.gnm_random_graph(N, M)\n",
    "degrees = [deg for _, deg in G.degree()]  # Original degree distribution\n",
    "\n",
    "# Traceroute sampling parameters\n",
    "n_sources = 8\n",
    "share_nodes = []\n",
    "share_edges = []\n",
    "sampled_degrees = []\n",
    "sampled_clustering = []\n",
    "N_targets = np.arange(50, 261, 50)\n",
    "\n",
    "# Perform traceroute sampling for different numbers of targets\n",
    "for n_targets in N_targets:\n",
    "    sampled_graph = traceroute_sampling(G, n_sources, n_targets)  # Updated function returns the graph directly\n",
    "\n",
    "    # Compute metrics\n",
    "    share_nodes.append(sampled_graph.number_of_nodes() / float(N))\n",
    "    share_edges.append(sampled_graph.number_of_edges() / float(M))\n",
    "    sampled_degrees.append([deg for _, deg in sampled_graph.degree()])\n",
    "    sampled_clustering.append(list(nx.clustering(sampled_graph).values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot share of nodes and edges\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.plot(N_targets, share_nodes, label='Share nodes')\n",
    "plt.plot(N_targets, share_edges, label='Share edges')\n",
    "plt.xlabel(\"Number of Targets\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend()\n",
    "plt.title(\"Share of Nodes and Edges by Number of Targets\")\n",
    "plt.show()\n",
    "\n",
    "# Plot degree distributions\n",
    "plt.figure(figsize=(16, 5))\n",
    "for k, values in enumerate(sampled_degrees):\n",
    "    x, y = get_binning(values, log_binning=True, num_bins=30)\n",
    "    plt.loglog(x, y, 'o', label=f\"Number of targets: {N_targets[k]}\")\n",
    "\n",
    "# Plot original degree distribution\n",
    "x, y = get_binning(degrees, log_binning=True, num_bins=30)\n",
    "plt.loglog(x, y, 'o', label=\"Original\")\n",
    "\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.title(\"Degree Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traceroute_sampling_revised(G, source_node, n_targets=50):\n",
    "    \"\"\"\n",
    "    Perform traceroute sampling from a single source to multiple targets.\n",
    "\n",
    "    Parameters:\n",
    "    G : networkx.Graph\n",
    "        Input graph.\n",
    "    source_node : node\n",
    "        Source node to start the traceroute.\n",
    "    n_targets : int, optional\n",
    "        Number of target nodes (default is 50).\n",
    "\n",
    "    Returns:\n",
    "    networkx.Graph\n",
    "        A new graph containing the sampled nodes and edges.\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    # Remove the source node from the list of potential targets\n",
    "    nodes.remove(source_node)\n",
    "    shuffle(nodes)\n",
    "\n",
    "    # Select targets\n",
    "    targets = set(nodes[:n_targets])\n",
    "\n",
    "    # Initialize the sampled graph\n",
    "    G_s = nx.Graph() if not G.is_directed() else nx.DiGraph()\n",
    "\n",
    "    # Compute shortest paths from the source to each target\n",
    "    for target_node in targets:\n",
    "        try:\n",
    "            path = nx.shortest_path(G, source=source_node, target=target_node)\n",
    "            G_s.add_nodes_from(path)\n",
    "            G_s.add_edges_from((path[i], path[i + 1]) for i in range(len(path) - 1))\n",
    "        except nx.NetworkXNoPath:\n",
    "            pass  # Ignore if no path exists\n",
    "\n",
    "    return G_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "\n",
    "# Parameters\n",
    "n_sources = 8\n",
    "n_targets = 100\n",
    "B = 200\n",
    "degree = defaultdict(list)\n",
    "\n",
    "# Run sampling iterations\n",
    "for b in range(B):\n",
    "#     print(f\"Iteration {b + 1}/{B}\")\n",
    "\n",
    "    # Shuffle and select source nodes\n",
    "    nodes = list(G.nodes())\n",
    "    np.random.shuffle(nodes)\n",
    "    source_nodes = nodes[:n_sources]\n",
    "\n",
    "    # Perform traceroute sampling for each source node\n",
    "    sampled_graphs = [traceroute_sampling_revised(G, source_node, n_targets) for source_node in source_nodes]\n",
    "\n",
    "    # Generate all pairs of sampled graphs\n",
    "    pairs = combinations(sampled_graphs, 2)\n",
    "\n",
    "    for sampled_graph1, sampled_graph2 in pairs:\n",
    "        for u in sampled_graph1.nodes:\n",
    "            # Get neighbors in both sampled graphs\n",
    "            neigh1 = set(sampled_graph1.neighbors(u))\n",
    "            if u in sampled_graph2:\n",
    "                neigh2 = set(sampled_graph2.neighbors(u))\n",
    "                # Compute the degree estimation based on shared neighbors\n",
    "                N_c = float(len(neigh1.intersection(neigh2)))\n",
    "                if N_c > 2:\n",
    "                    degree[u].append(len(neigh1) * len(neigh2) / N_c)\n",
    "\n",
    "# Compute degree estimates\n",
    "degree_est = [np.median(values) for values in degree.values()]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(16, 5))\n",
    "\n",
    "for k, values in enumerate(sampled_degrees):\n",
    "    x,y = get_binning(values, log_binning=True, num_bins=30)\n",
    "    __ = plt.loglog(x,y, 'o', label = \"Number of targets: %d\" % N_targets[k])\n",
    "\n",
    "# Original degree distribution\n",
    "x, y = get_binning(degrees, log_binning=True, num_bins=30)\n",
    "plt.loglog(x, y, '-o', label=\"Original\")\n",
    "\n",
    "# Bias-corrected degree estimates\n",
    "x, y = get_binning(degree_est, log_binning=True, num_bins=30)\n",
    "plt.loglog(x, y, '-o', label=\"Bias Correction\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walk Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_walk_sampling(G, n, n0=10, fly_back=0.15, jump_after=10):\n",
    "    \"\"\"\n",
    "    Perform random walk sampling on a graph.\n",
    "\n",
    "    Parameters:\n",
    "    G : networkx.Graph\n",
    "        Input graph.\n",
    "    n : int\n",
    "        Target number of nodes to sample.\n",
    "    n0 : int, optional\n",
    "        Number of initial walkers (default is 10).\n",
    "    fly_back : float, optional\n",
    "        Probability of flying back to the starting node (default is 0.15).\n",
    "    jump_after : int, optional\n",
    "        Number of consecutive steps stuck before restarting (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    networkx.Graph\n",
    "        A new graph G_s containing the sampled nodes and edges.\n",
    "    \"\"\"\n",
    "    # Initialize sampled nodes and edges\n",
    "    nodes = list(G.nodes())\n",
    "    shuffle(nodes)\n",
    "    V0 = nodes[:n0]  # Initial walker positions\n",
    "    V_s = set(V0)\n",
    "    E_s = set()\n",
    "    \n",
    "    # State variables for each walker\n",
    "    current_position = V0[:]\n",
    "    prev_position = [-1] * n0\n",
    "    stuck = [0] * n0\n",
    "\n",
    "    for k in range(n0):\n",
    "        while len(V_s) < n:\n",
    "            # Check if walker is stuck\n",
    "            if current_position[k] == prev_position[k]:\n",
    "                stuck[k] += 1\n",
    "            if stuck[k] > jump_after:\n",
    "                shuffle(nodes)\n",
    "                current_position[k] = nodes[0]\n",
    "                stuck[k] = 0\n",
    "\n",
    "            # Select a random neighbor for the next step\n",
    "            neighbors = list(G.neighbors(current_position[k]))\n",
    "            if not neighbors:  # Handle isolated nodes\n",
    "                continue\n",
    "            shuffle(neighbors)\n",
    "            \n",
    "            # Update positions\n",
    "            prev_position[k] = current_position[k]\n",
    "            current_position[k] = neighbors[0]\n",
    "\n",
    "            # Add the new node and edge to the sampled set\n",
    "            V_s.add(current_position[k])\n",
    "            edge = (current_position[k], prev_position[k]) if current_position[k] > prev_position[k] \\\n",
    "                else (prev_position[k], current_position[k])\n",
    "            E_s.add(edge)\n",
    "\n",
    "            # Fly back to the starting node with a given probability\n",
    "            if np.random.rand() <= fly_back:\n",
    "                current_position[k] = V0[k]\n",
    "\n",
    "    # Create the new sampled graph\n",
    "    G_s = nx.Graph() if not G.is_directed() else nx.DiGraph()\n",
    "    G_s.add_nodes_from(V_s)\n",
    "    G_s.add_edges_from(E_s)\n",
    "\n",
    "    return G_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "G = nx.barabasi_albert_graph(N,5)\n",
    "M = G.number_of_edges()\n",
    "degrees = list(dict(G.degree()).values())\n",
    "clustering = list(nx.clustering(G).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "n = 1000  # Target sample size\n",
    "n0 = 10  # Number of walkers\n",
    "fly_back = 0.15\n",
    "jump_after = 10\n",
    "\n",
    "degrees_sampled = []\n",
    "\n",
    "for k in range(1, 6):  # Number of waves or attempts\n",
    "    G_s = random_walk_sampling(G, n=n, n0=n0, fly_back=fly_back, jump_after=jump_after)\n",
    "    degrees_sampled.append([deg for _, deg in G_s.degree()])\n",
    "    x, y = get_binning(degrees_sampled[-1], log_binning=True, num_bins=15, is_pmf=False)\n",
    "    __ = plt.loglog(x, y, '--o', label=f'Sampling: Wave {k}')\n",
    "\n",
    "# Original graph degree distribution\n",
    "degrees = [deg for _, deg in G.degree()]\n",
    "x, y = get_binning(degrees, log_binning=True, num_bins=15, is_pmf=False)\n",
    "__ = plt.scatter(x, y, s=100, facecolors='none', edgecolors='r', lw=3, label='Original Graph')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings RW sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings_random_walk_sampling(G, n, n0=10, fly_back=0.0, jump_after=10):\n",
    "    \"\"\"\n",
    "    Perform Metropolis-Hastings random walk sampling on a graph.\n",
    "\n",
    "    Parameters:\n",
    "    G : networkx.Graph\n",
    "        Input graph.\n",
    "    n : int\n",
    "        Target number of nodes to sample.\n",
    "    n0 : int, optional\n",
    "        Number of initial walkers (default is 10).\n",
    "    fly_back : float, optional\n",
    "        Probability of flying back to the starting node (default is 0.0).\n",
    "    jump_after : int, optional\n",
    "        Number of consecutive steps stuck before restarting (default is 10).\n",
    "\n",
    "    Returns:\n",
    "    networkx.Graph\n",
    "        A new graph G_s containing the sampled nodes and edges.\n",
    "    \"\"\"\n",
    "    # Initialize sampled nodes and edges\n",
    "    nodes = list(G.nodes())\n",
    "    shuffle(nodes)\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    V0 = nodes[:n0]  # Initial walker positions\n",
    "    V_s = set(V0)\n",
    "    E_s = set()\n",
    "    \n",
    "    # State variables for each walker\n",
    "    current_position = V0[:]\n",
    "    prev_position = [-1] * n0\n",
    "    stuck = [0] * n0\n",
    "    \n",
    "    while len(V_s) < n:\n",
    "        for k in range(n0):\n",
    "            # Check if walker is stuck\n",
    "            if current_position[k] == prev_position[k]:\n",
    "                stuck[k] += 1\n",
    "            if stuck[k] > jump_after:\n",
    "                shuffle(nodes)\n",
    "                current_position[k] = nodes[0]\n",
    "                stuck[k] = 0\n",
    "\n",
    "            # Select a random neighbor\n",
    "            neighbors = list(G.neighbors(current_position[k]))\n",
    "            if not neighbors:  # Handle isolated nodes\n",
    "                continue\n",
    "            shuffle(neighbors)\n",
    "            \n",
    "            # Metropolis-Hastings condition\n",
    "            r = np.random.rand()\n",
    "            condition = degrees[current_position[k]] / float(degrees[neighbors[0]])\n",
    "            \n",
    "            if r <= condition:\n",
    "                prev_position[k] = current_position[k]\n",
    "                current_position[k] = neighbors[0]\n",
    "                V_s.add(current_position[k])\n",
    "                edge = (current_position[k], prev_position[k]) if current_position[k] >= prev_position[k] \\\n",
    "                    else (prev_position[k], current_position[k])\n",
    "                E_s.add(edge)\n",
    "\n",
    "                # Fly back to the starting node with a given probability\n",
    "                if np.random.rand() < fly_back:\n",
    "                    current_position[k] = V0[k]\n",
    "            \n",
    "            if len(V_s) >= n:\n",
    "                break\n",
    "        \n",
    "        if len(V_s) >= n:\n",
    "            break\n",
    "\n",
    "    # Create the new sampled graph\n",
    "    G_s = nx.Graph() if not G.is_directed() else nx.DiGraph()\n",
    "    G_s.add_nodes_from(V_s)\n",
    "    G_s.add_edges_from(E_s)\n",
    "\n",
    "    return G_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000\n",
    "G = nx.barabasi_albert_graph(N,5)\n",
    "M = G.number_of_edges()\n",
    "degrees = list(dict(G.degree()).values())\n",
    "clustering = list(nx.clustering(G).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "n = 3000  # Target sample size\n",
    "n0 = 10  # Number of walkers\n",
    "fly_back = 0.15\n",
    "jump_after = 10\n",
    "\n",
    "degrees_sampled = []\n",
    "\n",
    "for k in range(1, 6):  # Number of waves or attempts\n",
    "    G_s = metropolis_hastings_random_walk_sampling(G, n=n, n0=n0, fly_back=fly_back, jump_after=jump_after)\n",
    "    degrees_sampled.append([deg for _, deg in G_s.degree()])\n",
    "    x, y = get_binning(degrees_sampled[-1], log_binning=True, num_bins=15, is_pmf=False)\n",
    "    __ = plt.loglog(x, y, '--o', label=f'Sampling: Wave {k}')\n",
    "\n",
    "# Original graph degree distribution\n",
    "degrees = [deg for _, deg in G.degree()]\n",
    "x, y = get_binning(degrees, log_binning=True, num_bins=15, is_pmf=False)\n",
    "__ = plt.scatter(x, y, s=100, facecolors='none', edgecolors='r', lw=3, label='Original Graph')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different sampling algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_designs = ['induced_subgraph_sampling',\n",
    "                    'incident_subgraph_sampling',\n",
    "                    'edge_sampling_with_induction',\n",
    "#                     'labeled_star_sampling',\n",
    "                    'snowball_sampling',\n",
    "                    'traceroute_sampling',\n",
    "                    'random_walk_sampling', \n",
    "                    'metropolis_hastings_random_walk_sampling']\n",
    "sampling_edges = ['incident_subgraph_sampling',\n",
    "                  'edge_sampling_with_induction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label = 'BA graph'\n",
    "graph = G\n",
    "graph.remove_edges_from(nx.selfloop_edges(graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = graph.number_of_nodes()\n",
    "M = graph.number_of_edges()\n",
    "print(\"Number of nodes: %d\" % N)\n",
    "print(\"Number of edges: %d\" % M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = list(dict(graph.degree()).values())\n",
    "clustering = list(nx.clustering(graph).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage = 0.1\n",
    "n_v = int(N*coverage)\n",
    "n_e = int(M*coverage/2.5)\n",
    "snowball_v0 = 5\n",
    "n_waves = 2\n",
    "n_sources = 60\n",
    "n_targets = 1000\n",
    "n_labeled_star = int(N*coverage/2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 5  # Number of bootstrap iterations\n",
    "stats = dict()\n",
    "\n",
    "# Ensure sampling_designs contains the relevant sampling functions\n",
    "for sampling_label in sampling_designs:\n",
    "    sampling = eval(sampling_label)  # Get the function by its name\n",
    "    print('\\n', sampling_label)\n",
    "\n",
    "    # Initialize statistics\n",
    "    share_nodes = 0.0\n",
    "    share_edges = 0.0\n",
    "    ks_degree = 0.0\n",
    "    ks_clustering = 0.0\n",
    "\n",
    "    for b in range(B):\n",
    "        print(f\"Iteration {b + 1}/{B}\")\n",
    "\n",
    "        # Perform sampling and generate the sampled graph\n",
    "        if sampling_label in sampling_edges:\n",
    "            sampled_graph = sampling(graph, n_e)  # Outputs a graph\n",
    "        elif sampling_label == 'labeled_star_sampling':\n",
    "            sampled_graph = sampling(graph, n_labeled_star)\n",
    "        elif sampling_label == 'traceroute_sampling':\n",
    "            sampled_graph = sampling(graph, n_sources, n_targets)\n",
    "        elif sampling_label == 'snowball_sampling':\n",
    "            snowball_v1 = np.random.choice(list(graph.nodes()),size=snowball_v0, replace=False)\n",
    "            sampled_graph = sampling(graph, snowball_v1, n_waves)\n",
    "        else:\n",
    "            sampled_graph = sampling(graph, n_v)\n",
    "\n",
    "        # Calculate metrics\n",
    "        share_nodes += sampled_graph.number_of_nodes() / float(N) / float(B)\n",
    "        share_edges += sampled_graph.number_of_edges() / float(M) / float(B)\n",
    "\n",
    "        # Degree and clustering metrics\n",
    "        degrees_s = [deg for _, deg in sampled_graph.degree()]\n",
    "        clustering_s = list(nx.clustering(sampled_graph).values())\n",
    "\n",
    "        ks_degree += ks_d(degrees, degrees_s) / float(B)\n",
    "        ks_clustering += ks_d(clustering, clustering_s) / float(B)\n",
    "\n",
    "    # Store the results for the current sampling method\n",
    "    stats[sampling_label] = {\n",
    "        'ks_degree': ks_degree,\n",
    "        'ks_clustering': ks_clustering,\n",
    "        'share_nodes': share_nodes,\n",
    "        'share_edges': share_edges\n",
    "    }\n",
    "\n",
    "# Display final statistics\n",
    "for label, metrics in stats.items():\n",
    "    print(f\"\\nSampling Method: {label}\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "for key, values in sorted(stats.items(), key = lambda x:x[1]['ks_degree']):\n",
    "    row = [key]\n",
    "    for k in ['share_nodes','share_edges','ks_degree','ks_clustering']:\n",
    "        row += ['%1.3f' % values[k]]\n",
    "    table.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(table, columns=['Sampling method','share_nodes','share_edges','ks_degree','ks_clustering'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network reconstruction using SBM with graph-tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "networkinf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
